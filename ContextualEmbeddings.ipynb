{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import glob\n",
    "import getpass\n",
    "from typing import List, Tuple\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters.markdown import MarkdownHeaderTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 markdown files.\n"
     ]
    }
   ],
   "source": [
    "# Folder containing the .md files\n",
    "folder_path = \"docs/data\"  # Change this to your actual folder path\n",
    "vector_db_folder = \"vector_db\"\n",
    "os.makedirs(vector_db_folder, exist_ok=True)\n",
    "\n",
    "# Get the first 5 .md files (sorted alphabetically)\n",
    "md_files = sorted(glob.glob(os.path.join(folder_path, \"*.md\")))[:5]\n",
    "\n",
    "# Dictionary to store file names and their contents\n",
    "md_dict = {}\n",
    "\n",
    "# Read each file and store in dictionary\n",
    "for file in md_files:\n",
    "    file_name = os.path.basename(file)  # Extract file name\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        md_dict[file_name] = f.read()\n",
    "\n",
    "print(f\"Loaded {len(md_dict)} markdown files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualRetrieval:\n",
    "    \"\"\"\n",
    "    A class that implements the Contextual Retrieval system.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the ContextualRetrieval system.\n",
    "        \"\"\"\n",
    "        self.text_splitter = MarkdownHeaderTextSplitter(\n",
    "            headers_to_split_on=[\n",
    "                (\"#\", \"Header 1\"),\n",
    "                (\"##\", \"Header 2\"),\n",
    "                (\"###\", \"Header 3\")\n",
    "            ]\n",
    "        )\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0,\n",
    "            max_tokens=None,\n",
    "            timeout=None,\n",
    "            max_retries=2,\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "\n",
    "    def process_documents(self, md_dict: dict) -> None:\n",
    "        \"\"\"\n",
    "        Process markdown files stored in md_dict and save them separately in the vector_db folder.\n",
    "        \"\"\"\n",
    "        for file_name, document in md_dict.items():\n",
    "            chunks, contextualized_chunks = self._process_single_document(document)\n",
    "            vector_store = self.create_vectorstores(contextualized_chunks)\n",
    "            vector_store.save_local(os.path.join(vector_db_folder, file_name.replace(\".md\", \"\")))\n",
    "        print(\"Documents processed and stored in vector_db folder.\")\n",
    "\n",
    "    def _process_single_document(self, document: str) -> Tuple[List[Document], List[Document]]:\n",
    "        \"\"\"\n",
    "        Process a single document by splitting it into chunks and generating contextualized versions.\n",
    "        \"\"\"\n",
    "        chunks = self.text_splitter.split_text(document)  # Ensure `document` is a string\n",
    "        contextualized_chunks = self._generate_contextualized_chunks(document, chunks)\n",
    "        return chunks, contextualized_chunks\n",
    "\n",
    "    def _generate_contextualized_chunks(self, document: str, chunks: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Generate contextualized versions of the given chunks.\n",
    "        \"\"\"\n",
    "        contextualized_chunks = []\n",
    "        for chunk in chunks:\n",
    "            context = self._generate_context(document, chunk.page_content)\n",
    "            contextualized_content = f\"{context}\\n\\n{chunk.page_content}\"\n",
    "            contextualized_chunks.append(Document(page_content=contextualized_content, metadata=chunk.metadata))\n",
    "        return contextualized_chunks\n",
    "\n",
    "    def create_vectorstores(self, chunks: List[Document]) -> FAISS:\n",
    "        \"\"\"\n",
    "        Create a vector store for the given chunks.\n",
    "        \"\"\"\n",
    "        return FAISS.from_documents(chunks, self.embeddings)\n",
    "\n",
    "    def _generate_context(self, document: str, chunk: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate context for a specific chunk using the language model.\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        ### Instructions:\n",
    "        1. **Understand the Documentâ€™s Context**:\n",
    "        - The purpose is to generate a short, precise context for the given chunk to improve search retrieval.\n",
    "\n",
    "        2. **Generate Context Without Redundant Phrasing**:\n",
    "        - Do **not** use phrases like \"This chunk discusses\" or \"This section provides.\"\n",
    "        - Directly state what the chunk is about in a clear and concise manner.\n",
    "\n",
    "        3. **Handle Tables Properly (if applicable)**:\n",
    "        - If the chunk contains a table, summarize its purpose and what kind of data it represents.\n",
    "\n",
    "        4. **Keep It Brief & Informative**:\n",
    "        - The context should be **3 to 6 sentences max**.\n",
    "        - It should provide just enough information to situate the chunk within the document while remaining succinct.\n",
    "\n",
    "        **Answer only with the succinct context and nothing else.**\n",
    "\n",
    "        <document>\n",
    "        {document}\n",
    "        </document>\n",
    "\n",
    "        Here is the chunk we want to situate within the whole document:\n",
    "        <chunk>\n",
    "        {chunk}\n",
    "        </chunk>\n",
    "\n",
    "        ### **Context:**\n",
    "        \"\"\")\n",
    "\n",
    "        messages = prompt.format_messages(document=document, chunk=chunk)\n",
    "        response = self.llm.invoke(messages)\n",
    "        return response.content\n",
    "\n",
    "    def query_multiple_vectorstores(self, query: str, top_k: int = 3) -> List[str]:\n",
    "        \"\"\"\n",
    "        Query multiple vector stores in the vector_db folder and retrieve the most relevant results.\n",
    "\n",
    "        :param query: The query string.\n",
    "        :param top_k: Number of top results to retrieve from each vector store.\n",
    "        :return: List of retrieved responses.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        # Iterate through each stored vector database\n",
    "        for file_name in os.listdir(vector_db_folder):\n",
    "            vector_store_path = os.path.join(vector_db_folder, file_name)\n",
    "            if os.path.isdir(vector_store_path):\n",
    "                # Load the vector store\n",
    "                vector_store = FAISS.load_local(vector_store_path, self.embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "                # Perform similarity search\n",
    "                retrieved_docs = vector_store.similarity_search(query, k=top_k)\n",
    "\n",
    "                # Extract content from retrieved documents\n",
    "                for doc in retrieved_docs:\n",
    "                    results.append(doc.page_content)\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "    def generate_answer(self, query: str, relevant_chunks: List[str]) -> str:\n",
    "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Based on the following information, please provide a concise and accurate answer to the question.\n",
    "        If the information is not sufficient to answer the question, say so.\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Relevant information:\n",
    "        {chunks}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\")\n",
    "        messages = prompt.format_messages(query=query, chunks=\"\\n\\n\".join(relevant_chunks))\n",
    "        response = self.llm.invoke(messages)\n",
    "        return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_system = ContextualRetrieval()\n",
    "retrieval_system.process_documents(md_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retrieval_system.query_multiple_vectorstores(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextualized_vector_answer = retrieval_system.generate_answer(query, docs)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
